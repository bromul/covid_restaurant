{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data from [Yelp Dataset JSON](https://www.yelp.com/dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Import NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and Open data file\n",
    "data_file = open('../../Data/archive/yelp_academic_dataset_review.json', encoding = 'utf8')\n",
    "data = []\n",
    "\n",
    "# Read in data\n",
    "for line in data_file:\n",
    "    data.append(json.loads(line))\n",
    "\n",
    "review_df = pd.DataFrame(data)\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop reviews from before 1 June 2019 at midnight\n",
    "review_df = review_df[review_df['date'] >= '2019-06-01 00:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort reviews temporally\n",
    "review_df = review_df.sort_values(by = 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set stop words\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all positive reviews and reset index\n",
    "bad_review_df = review_df[review_df['stars'] < 3.0].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_text = nltk.Text(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_text.concordance('artichoke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "bad_review_df.drop(['review_id', 'user_id', 'useful', 'funny', 'cool'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_review_df.iloc[[10000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only the review coulumn\n",
    "reviews = bad_review_df['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to ensure each word is lemmatized as the proper part of speech\n",
    "def lemma_pos(text):\n",
    "    \n",
    "    # Dependencies\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    from nltk import word_tokenize, pos_tag\n",
    "    from collections import defaultdict\n",
    "\n",
    "    # Create tag map\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "\n",
    "    # Initialize list\n",
    "    lemmas = []\n",
    "\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        lemma = lmtzr.lemmatize(token, tag_map[tag[0]])\n",
    "        lemmas.append(lemma)\n",
    "    \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to word tokenize and lemmatize each review, then check for sense words\n",
    "\n",
    "def create_word_list(review):\n",
    "    \n",
    "    # Define stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Word tokenize review\n",
    "    review_words = word_tokenize(review)\n",
    "\n",
    "    # Filter words \n",
    "    filtered_words = []\n",
    "\n",
    "    filtered_words = [\n",
    "        word for word in review_words if word.casefold() not in stop_words\n",
    "    ]\n",
    "\n",
    "    # Lemmatize filtered words\n",
    "    lemmas = lemmas_pos(filtered_words)\n",
    "    \n",
    "    # Check for sense words\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8ee7bb5d06fefd70f8a98d7345ee36eea54533999c2e9150ed66738d5a0c643"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
